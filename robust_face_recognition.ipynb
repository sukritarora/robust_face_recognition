{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:06:41.023184Z",
     "start_time": "2020-12-17T07:06:41.013361Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from glob import glob as glob\n",
    "import os\n",
    "import pywt\n",
    "import cv2\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T23:53:18.130093Z",
     "start_time": "2020-12-16T23:53:18.127449Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_class = 15\n",
    "# num_train = 20\n",
    "# num_test = 5\n",
    "im_size = np.array((192, 168))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:08:06.976969Z",
     "start_time": "2020-12-17T07:08:06.956688Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_files(fnames, cond, size=5):\n",
    "    fnames_base = [os.path.basename(i)[:-4] for i in fnames]\n",
    "    acc_files = []\n",
    "    for i, f in enumerate(fnames_base):\n",
    "        az = int(f[12:16])\n",
    "        elev = int(f[17:20])\n",
    "        if cond(az, elev):\n",
    "#             print(az, elev)\n",
    "            acc_files.append(fnames[i])\n",
    "#     samps = np.random.choice(acc_files, size, replace=False)\n",
    "    return acc_files\n",
    "\n",
    "def fread(f):\n",
    "    return plt.imread(f).flatten().T\n",
    "\n",
    "def random_sample_cond(train_cond, test_cond):\n",
    "    \n",
    "    all_fnames = glob(\"*/*/*_P00A*.pgm\")\n",
    "    \n",
    "    train_fnames = filter_files(all_fnames, train_cond) \n",
    "    test_fnames = filter_files(all_fnames, test_cond)\n",
    "    \n",
    "    num_train = len(train_fnames)\n",
    "    num_test = len(test_fnames)\n",
    "    \n",
    "    A = np.zeros((np.prod(im_size), num_train))\n",
    "    y = np.zeros((np.prod(im_size), num_test))\n",
    "    \n",
    "    train_gt = np.zeros(num_train).astype(int)\n",
    "    test_gt = np.zeros(num_test).astype(int)\n",
    "    \n",
    "    for i, f in enumerate(train_fnames):\n",
    "        A[:,i] = fread(f)\n",
    "        train_gt[i] = int(os.path.basename(f)[5:7])-1\n",
    "        \n",
    "    for i, f in enumerate(test_fnames):\n",
    "        y[:,i] = fread(f)\n",
    "        test_gt[i] = int(os.path.basename(f)[5:7])-1\n",
    "            \n",
    "    return A, y, train_gt, test_gt, (train_fnames, test_fnames)\n",
    "\n",
    "def random_sample():\n",
    "    all_fnames = glob(\"*/*/*_P00A*.pgm\")\n",
    "    half = len(all_fnames)//2\n",
    "    np.random.shuffle(all_fnames)\n",
    "    \n",
    "    train_fnames = all_fnames[:half]\n",
    "    test_fnames = all_fnames[half:]\n",
    "    \n",
    "    A = np.zeros((np.prod(im_size), len(train_fnames)))\n",
    "    y = np.zeros((np.prod(im_size), len(test_fnames)))\n",
    "    \n",
    "    train_gt = np.zeros(len(train_fnames)).astype(int)\n",
    "    test_gt = np.zeros(len(test_fnames)).astype(int)\n",
    "    \n",
    "    for i, f in enumerate(train_fnames):\n",
    "        A[:,i] = fread(f)\n",
    "        train_gt[i] = int(os.path.basename(f)[5:7])-1\n",
    "        \n",
    "    for i, f in enumerate(test_fnames):\n",
    "        y[:,i] = fread(f)\n",
    "        test_gt[i] = int(os.path.basename(f)[5:7])-1\n",
    "        \n",
    "    return A, y, train_gt, test_gt, (train_fnames, test_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:08:23.011410Z",
     "start_time": "2020-12-17T07:08:17.963495Z"
    }
   },
   "outputs": [],
   "source": [
    "A, y, train_gt, test_gt, names = random_sample()\n",
    "print(A.shape, y.shape)\n",
    "print(train_gt)\n",
    "print(test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:13:51.293095Z",
     "start_time": "2020-12-17T07:13:51.281605Z"
    }
   },
   "outputs": [],
   "source": [
    "def down_samp(A, ds_factor=16):\n",
    "    im_size_down = np.ceil(im_size/ds_factor).astype(int)\n",
    "    A_down = np.zeros((np.prod(im_size_down), A.shape[-1]))\n",
    "#     print(A_down.shape)\n",
    "    for i in range(A.shape[-1]):\n",
    "        A_down[:,i] = A[:,i].reshape(im_size)[::ds_factor, ::ds_factor].flatten()\n",
    "    return A_down, im_size_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T23:53:32.893622Z",
     "start_time": "2020-12-16T23:53:32.890035Z"
    }
   },
   "outputs": [],
   "source": [
    "def down_samp_pca(A, dim=132):\n",
    "    # sklearn PCA\n",
    "    pca = PCA(n_components=dim, svd_solver=\"auto\")\n",
    "    A_pca = pca.fit_transform(A.T).T\n",
    "    \n",
    "    # Manual PCA\n",
    "#     U, S, Vh = np.linalg.svd(A, full_matrices=True)\n",
    "#     print(U.shape, S.shape, Vh.shape)\n",
    "#     A_pca = U[:,:dim].T@A\n",
    "    return A_pca, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down(A, x, down_samp_func):\n",
    "    return down_samp_func(A,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:28:08.702945Z",
     "start_time": "2020-12-17T07:28:08.196802Z"
    }
   },
   "outputs": [],
   "source": [
    "pywt.wavedec2(A.reshape((*im_size,-1)), 'haar', axes=(0,1), level=1)[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:28:14.807759Z",
     "start_time": "2020-12-17T07:28:14.802626Z"
    }
   },
   "outputs": [],
   "source": [
    "def down_samp_wave(A, level=4):\n",
    "    im_vec = A.reshape((*im_size,-1))\n",
    "    wave_vec = pywt.wavedec2(im_vec, 'haar', axes=(0,1), level=level)\n",
    "#     low_dim_data = wave_vec[0]\n",
    "    low_dim_data = ((wave_vec[0] + sum(wave_vec[1]))/4)\n",
    "#     low_dim_data = wave_vec[0] + sum(wave_vec[1])\n",
    "    shape = low_dim_data.shape[:2]\n",
    "    low_dim_data = low_dim_data.reshape(-1, A.shape[-1])\n",
    "    quantized_data = (low_dim_data/np.max(low_dim_data, axis=0)*255).astype(np.uint8)\n",
    "    return quantized_data, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_samp_CAE(A, net):\n",
    "    t = np_to_torch(A)\n",
    "    with torch.no_grad():\n",
    "        low_dim, out = net(t)\n",
    "        imgs = (low_dim.cpu().numpy()).squeeze()\n",
    "    res = imgs.reshape((-1, np.prod(imgs.shape[1:]))).T\n",
    "    return res, res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T23:53:35.820130Z",
     "start_time": "2020-12-16T23:53:35.816124Z"
    }
   },
   "outputs": [],
   "source": [
    "def down_samp_cv(A):\n",
    "    A_down = np.zeros((120, A.shape[-1]))\n",
    "    for i in range(A.shape[-1]):\n",
    "        A_down[:,i] = cv2.resize(A[:,i].reshape(im_size), (11,12)).flatten()\n",
    "    return A_down, (11,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"CAE_100\", map_location=torch.device('cpu'))\n",
    "down_samp_CAE(A, net).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:51:09.484155Z",
     "start_time": "2020-12-17T06:51:09.475774Z"
    }
   },
   "outputs": [],
   "source": [
    "A_ds, ds_shape = down_samp(A, ds_factor=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:56:59.176693Z",
     "start_time": "2020-12-17T06:56:57.454862Z"
    }
   },
   "outputs": [],
   "source": [
    "A_wave, wave_shape = down_samp_wave(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ds_cv, ds_cv_shape = down_samp_cv(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:57:00.638567Z",
     "start_time": "2020-12-17T06:57:00.412350Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(A[...,0].reshape(im_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(A_ds_cv[...,0].reshape(ds_cv_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:57:02.511732Z",
     "start_time": "2020-12-17T06:57:02.370271Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(A_ds[...,0].reshape(ds_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:57:04.215792Z",
     "start_time": "2020-12-17T06:57:04.211448Z"
    }
   },
   "outputs": [],
   "source": [
    "A_wave.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:57:11.247763Z",
     "start_time": "2020-12-17T06:57:11.106919Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(A_wave[...,1].reshape(wave_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T06:54:27.771007Z",
     "start_time": "2020-12-16T06:54:27.262600Z"
    }
   },
   "outputs": [],
   "source": [
    "A_pca, _ = down_samp_pca(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T06:54:27.798697Z",
     "start_time": "2020-12-16T06:54:27.794533Z"
    }
   },
   "outputs": [],
   "source": [
    "A_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T06:54:30.048353Z",
     "start_time": "2020-12-16T06:54:29.932692Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(A_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T21:21:08.572648Z",
     "start_time": "2020-12-16T21:21:08.569013Z"
    }
   },
   "outputs": [],
   "source": [
    "# def delta(x, i):\n",
    "#     assert i < num_class\n",
    "#     out = np.zeros(len(x))\n",
    "#     idxs = slice(i*num_class, i*num_class+num_train)\n",
    "#     out[idxs] = x[idxs]\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T23:53:43.108221Z",
     "start_time": "2020-12-16T23:53:43.105078Z"
    }
   },
   "outputs": [],
   "source": [
    "def delta_i(x, i, gt):\n",
    "    return np.where(gt==i, x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T23:53:44.087933Z",
     "start_time": "2020-12-16T23:53:44.081668Z"
    }
   },
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def identity(A, y, class_idxs, lmbda=1e-12):\n",
    "    A_norm = np.linalg.norm(A, axis=0)\n",
    "    y_norm = np.linalg.norm(y)\n",
    "#     print(A, y)\n",
    "    prob = Lasso(fit_intercept=False, alpha=lmbda, max_iter=1e3)\n",
    "#     prob = LassoCV(fit_intercept=False, max_iter=1e4)\n",
    "    prob.fit(A/A_norm, y/y_norm)\n",
    "    x_hat = prob.coef_\n",
    "    r = np.zeros(38)\n",
    "    for i in range(38):\n",
    "        r[i] = np.linalg.norm(y-A@delta_i(x_hat, i, class_idxs))\n",
    "#     print(x_hat)\n",
    "    return np.argmin(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:45:43.735823Z",
     "start_time": "2020-12-17T06:45:43.725415Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(A, y, train_gt, test_gt, ld_func=down_samp, size_arg = None, lmbda=1e-12, train=True):\n",
    "    train_pred = np.ones_like(train_gt)*-1\n",
    "    test_pred = np.ones_like(test_gt)*-1\n",
    "    \n",
    "    if ld_func.__name__ == \"down_samp_pca\":\n",
    "        A_ld, pca = ld_func(A, size_arg)\n",
    "        y_ld = pca.transform(y.T).T\n",
    "    else:\n",
    "        A_ld, _ = ld_func(A, size_arg)\n",
    "        y_ld, _ = ld_func(y, size_arg)\n",
    "    \n",
    "#     print(A_ld.shape, y_ld.shape)\n",
    "    if train:\n",
    "        for i in tqdm(range(len(train_pred))):\n",
    "            train_pred[i] = identity(A_ld, A_ld[:,i], train_gt, lmbda)\n",
    "#         print(train_pred[i], train_gt[i])\n",
    "    \n",
    "    for i in tqdm(range(len(test_pred)), position=0, leave=True):\n",
    "        test_pred[i] = identity(A_ld, y_ld[:,i], train_gt, lmbda)\n",
    "    \n",
    "#     print(train_pred)\n",
    "    train_acc = accuracy_score(train_gt, train_pred)*100\n",
    "    test_acc = accuracy_score(test_gt, test_pred)*100\n",
    "    \n",
    "    if train:\n",
    "        print(\"Accuracy for {}:\\n\\tTrain Accuracy: {:.2f}\\n\\tTest Accuracy: {:.2f}\".format(ld_func.__name__, train_acc, test_acc))\n",
    "    else:\n",
    "        print(\"Accuracy for {}:\\n\\tTest Accuracy: {:.2f}\".format(ld_func.__name__, test_acc))\n",
    "        \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:46:51.727304Z",
     "start_time": "2020-12-17T06:46:51.724524Z"
    }
   },
   "outputs": [],
   "source": [
    "down_samp_funcs = [down_samp, down_samp_wave,  down_samp_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:57:42.688822Z",
     "start_time": "2020-12-17T06:57:29.318635Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for func in down_samp_funcs:\n",
    "    evaluate(A, y, train_gt, test_gt, ld_func=func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T05:46:51.124254Z",
     "start_time": "2020-12-17T05:46:51.087424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Corrupt data for robust identity\n",
    "def corrupt(test_mat, percent=0.3):\n",
    "    im_size, num_ims = test_mat.shape\n",
    "    corrupt_mat = test_mat.copy()\n",
    "    corrupt_size = int(im_size*percent)\n",
    "    for i in range(num_ims):\n",
    "        corrupt_idxs = np.random.choice(im_size, size=corrupt_size, replace=False)\n",
    "        corrupt_data = np.random.randint(256, size=corrupt_size)\n",
    "        corrupt_mat[:,i][corrupt_idxs] = corrupt_data\n",
    "#     print(im_size, num_ims, corrupt_size)\n",
    "    return corrupt_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:28:22.832367Z",
     "start_time": "2020-12-17T07:28:22.426000Z"
    }
   },
   "outputs": [],
   "source": [
    "y_ds, y_ds_sh = down_samp(y)\n",
    "y_wave, y_wave_sh = down_samp_wave(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:28:23.696172Z",
     "start_time": "2020-12-17T07:28:23.069899Z"
    }
   },
   "outputs": [],
   "source": [
    "perc = 0.4\n",
    "k = 1\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(y_ds[:,k].reshape(y_ds_sh))\n",
    "plt.subplot(222)\n",
    "plt.imshow(corrupt(y_ds, perc)[:,k].reshape(y_ds_sh))\n",
    "plt.subplot(223)\n",
    "plt.imshow(y_wave[:,k].reshape(y_wave_sh))\n",
    "plt.subplot(224)\n",
    "plt.imshow(corrupt(y_wave, perc)[:,k].reshape(y_wave_sh))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def robust_identity(A, B, y, class_idxs, lmbda=1e-12, verbose = False):\n",
    "#     A_norm = np.linalg.norm(A, axis=0)\n",
    "#     y_norm = np.linalg.norm(y)\n",
    "    m,n = A.shape\n",
    "#     B = np.hstack((A, np.eye(m)))\n",
    "#     B_norm = np.linalg.norm(B, axis=0)\n",
    "#     print(B.shape)\n",
    "    prob = Lasso(fit_intercept=False, alpha=1e-12)\n",
    "    prob.fit(B, y)\n",
    "    w_hat = prob.coef_\n",
    "    x_hat = w_hat[:n]\n",
    "    e_hat = w_hat[n:]\n",
    "    r = np.zeros(38)\n",
    "    if verbose:\n",
    "        print(f\"Argwhere x_hat: {np.argwhere(x_hat>0.2)}\")\n",
    "    for i in range(38):\n",
    "        r[i] = np.linalg.norm(y-e_hat-A@delta_i(x_hat, i, class_idxs))\n",
    "    return np.argmin(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:36:05.204563Z",
     "start_time": "2020-12-17T07:36:05.196064Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_robust(A, y, train_gt, test_gt, ld_func=down_samp, size_arg = None, lmbda = 1e-12, verbose = False):\n",
    "    \n",
    "    test_pred = np.ones_like(test_gt)*-1\n",
    "    \n",
    "    if ld_func.__name__ == \"down_samp_pca\":\n",
    "        A_ld, pca = ld_func(A, size_arg)\n",
    "        y_ld = pca.transform(y.T).T\n",
    "    else:\n",
    "        A_ld, _ = ld_func(A)\n",
    "        y_ld, _ = ld_func(y)\n",
    "    if verbose:\n",
    "        print(f\"Low Dim Shapes\\n\\tA: {A_ld.shape}\\n\\ty:{y_ld.shape}\")\n",
    "\n",
    "#     corrupt_percs = np.arange(0,1,0.1)\n",
    "#     corrupt_percs = [0, 0.4, 0.6]\n",
    "    corrupt_percs = [0]\n",
    "    test_acc = []\n",
    "    \n",
    "    m,n = A_ld.shape\n",
    "    B = np.hstack((A_ld, np.eye(m)))\n",
    "    B = B/np.linalg.norm(B, axis=0)\n",
    "    A_ld = A_ld/np.linalg.norm(A_ld, axis=0)\n",
    "    for perc in corrupt_percs:\n",
    "        corrupt_y = corrupt(y_ld, perc)\n",
    "        corrupt_y = corrupt_y/np.linalg.norm(corrupt_y, axis=0)\n",
    "        for i in tqdm(range(len(test_pred)), position=0, leave=True):\n",
    "            test_pred[i] = robust_identity(A_ld, B, corrupt_y[:,i],train_gt,lmbda, verbose)\n",
    "\n",
    "    #     print(train_pred)\n",
    "        test_acc.append(accuracy_score(test_gt, test_pred)*100)\n",
    "        \n",
    "    \n",
    "    print(\"Robust Identity Accuracy for {}:\\n\\tTest Accuracy: {}\".format(ld_func.__name__, test_acc))\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:36:52.103554Z",
     "start_time": "2020-12-17T07:36:06.441405Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Change train and test data for robust\n",
    "#TODO: Change ds factor for robust# Robust data read\n",
    "train_cond = lambda az, elev: abs(az) <= 25 and abs(elev) <= 25\n",
    "test_cond = lambda az, elev: 25 <= abs(az) <= 100 and 25 <= abs(elev) <= 65\n",
    "\n",
    "A_cond, y_cond, train_gt_cond, test_gt_cond, names_cond = random_sample_cond(train_cond, test_cond)\n",
    "samp_idxs = np.random.choice(len(test_gt_cond), size=100)\n",
    "y_cond_samp = y_cond[:,samp_idxs]\n",
    "test_gt_cond_samp = test_gt_cond[samp_idxs]\n",
    "\n",
    "down_samp_funcs = {down_samp: [2, 4, 8, 16], down_samp_wave: [1, 2, 3, 4], down_samp_pca: [132, 504, 2016, 8064]}\n",
    "lmbdas = [1e-12, 1e-9, 1e-6, 1e-3]\n",
    "test_accs_normal = {\"down_samp\": [], \"down_samp_wave\": [], \"down_samp_pca\": []}\n",
    "for func, levels in down_samp_funcs.items():\n",
    "    for i, level in enumerate(levels):\n",
    "        test_accs_normal[func.__name__].append(evaluate(A_cond, y_cond_samp, \n",
    "                                                   train_gt, test_gt_cond_samp,\n",
    "                                                   ld_func=func, size_arg = level,\n",
    "                                                   lmbda=lmbdas[i], train=False))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T07:28:23.696172Z",
     "start_time": "2020-12-17T07:28:23.069899Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(A_cond, y_cond_samp, \n",
    "           train_gt, test_gt_cond_samp,\n",
    "           ld_func=down_samp_pca, size_arg = 504,\n",
    "           lmbda=9e-6, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs_normal = {\"down_samp\": [72.0, 72.0, 61.0, 35.0], \n",
    "                    \"down_samp_wave\": [74.0, 75.0, 67, 30.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, y, train_gt, test_gt, names = random_sample()\n",
    "samp_idxs = np.random.choice(len(test_gt), size=100)\n",
    "y_samp = y[:,samp_idxs]\n",
    "test_gt_samp = test_gt[samp_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change train and test data for robust\n",
    "#TODO: Change ds factor for robust# Robust data read\n",
    "\n",
    "\n",
    "test_accs = {\"down_samp\": [], \"down_samp_wave\": []}\n",
    "for func, levels in down_samp_funcs.items():\n",
    "    for i, level in enumerate(levels):\n",
    "        test_accs[func.__name__].append(evaluate(A, y_samp, \n",
    "                                                   train_gt, test_gt_samp,\n",
    "                                                   ld_func=func, size_arg = level,\n",
    "                                                   lmbda=lmbdas[i], train=False))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A, y, train_gt, test_gt, names = random_sample()\n",
    "fxns = {down_samp: 16, down_samp_wave: 4, down_samp_pca: 132, down_samp_CAE: net}\n",
    "for func, level in fxns.items():\n",
    "    evaluate(A, y, \n",
    "               train_gt, test_gt,\n",
    "               ld_func=func, size_arg = level,\n",
    "               lmbda=1e-5, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2\n",
    "evaluate(A, y, \n",
    "           train_gt, test_gt,\n",
    "           ld_func=down_samp_CAE, size_arg = net2,\n",
    "           lmbda=1e-5, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = A_cond[:,0].reshape(im_size)\n",
    "img_d = cv2.resize(img, (11,12))\n",
    "img_u = cv2.resize(img, im_size[::-1])\n",
    "plt.imshow(img_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.resize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T16:18:45.529872Z",
     "start_time": "2020-12-17T16:18:45.512357Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T16:45:21.889327Z",
     "start_time": "2020-12-17T16:45:21.885852Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T16:13:38.778085Z",
     "start_time": "2020-12-17T16:13:38.767389Z"
    }
   },
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super().__init__()\n",
    "        convs = []\n",
    "        deconvs = []\n",
    "        \n",
    "        strides = [1, 2]\n",
    "        kerns = [3,4]\n",
    "        convs.append(nn.Conv2d(1, N[0], kernel_size=3, stride=1, padding=1))\n",
    "        for i in range(len(N)-1):\n",
    "            s = strides[(i+1)%2]\n",
    "            convs.append(nn.PReLU())\n",
    "            convs.append(nn.Conv2d(N[i], N[i+1], kernel_size=3, stride=s, padding=1))\n",
    "        \n",
    "        for i in range(len(N)-1, 0, -1):\n",
    "            s = strides[(i+1)%2]\n",
    "            k = kerns[(i+1)%2]\n",
    "#             s = strides[i%2]\n",
    "#             k = kerns[i%2]\n",
    "            deconvs.append(nn.ConvTranspose2d(N[i], N[i-1], kernel_size=k, stride=s, padding=1))\n",
    "            deconvs.append(nn.PReLU())\n",
    "        deconvs.append(nn.ConvTranspose2d(N[0],1, kernel_size=4, stride=2, padding=1)) \n",
    "        \n",
    "        self.encoder = nn.Sequential(*convs)\n",
    "        self.decoder = nn.Sequential(*deconvs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        low_dim = self.encoder(x)\n",
    "        recon = self.decoder(low_dim)\n",
    "        return low_dim, recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T16:21:56.730205Z",
     "start_time": "2020-12-17T16:21:56.726829Z"
    }
   },
   "outputs": [],
   "source": [
    "# CAE([32, 32, 64, 64, 64, 32, 16, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T16:22:03.533524Z",
     "start_time": "2020-12-17T16:21:59.543073Z"
    }
   },
   "outputs": [],
   "source": [
    "A, y, train_gt, test_gt, names = random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(x):\n",
    "    x_ims = x.reshape((*im_size, -1))\n",
    "    x_ims_resize = np.zeros((192, 176, x_ims.shape[-1]))\n",
    "    for i in range(x_ims.shape[-1]):\n",
    "        x_ims_resize[...,i] = cv2.resize(x_ims[...,i], (176, 192))\n",
    "    return torch.from_numpy(x_ims_resize.transpose(2, 0, 1)[:, None, ...]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T16:28:00.524587Z",
     "start_time": "2020-12-17T16:28:00.519842Z"
    }
   },
   "outputs": [],
   "source": [
    "nn_data = np_to_torch(A)\n",
    "nn_dataset = TensorDataset(nn_data, nn_data)\n",
    "data_loader = DataLoader(dataset = nn_dataset, batch_size = 16, shuffle = True)\n",
    "nn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [32, 32, 64, 64, 64, 32, 16, 1]\n",
    "# N = [32, 32, 64, 64, 64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CAE(N).to(device)\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T16:28:06.358435Z",
     "start_time": "2020-12-17T16:28:06.355230Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T16:46:00.196877Z",
     "start_time": "2020-12-17T16:45:56.037335Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0\n",
    "    for i, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        low_dim, out = net(data.to(device))\n",
    "#         print(low_dim.shape, out.shape, target.shape)\n",
    "        loss = criterion(out, target.to(device)) #+ low_dim.norm()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(running_loss/i)\n",
    "    print(\"Epoch {} | Loss: {}\".format(epoch, running_loss/i), end=\"\\r\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net, \"CAE_100\")\n",
    "# np.save(\"CAE_100_loss\", np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = np_to_torch(y)\n",
    "test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T16:14:55.910474Z",
     "start_time": "2020-12-17T16:14:52.990Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    low_dim, out = net(test_tensor[10:11].to(device))\n",
    "    res = out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T10:26:01.196482Z",
     "start_time": "2020-12-17T10:26:01.185982Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(res.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T10:24:32.547547Z",
     "start_time": "2020-12-17T10:24:32.532632Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.linalg.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
